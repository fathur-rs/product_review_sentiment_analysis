{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASSES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reviews:\n",
    "    def __init__(self, text ,score, sentiment):\n",
    "        self.text = text\n",
    "        self.score = score\n",
    "        self.sentiment = sentiment\n",
    "\n",
    "    def get_sentiment(self):\n",
    "        if self.score <= 2:\n",
    "            return self.sentiment\n",
    "        elif self.score == 3:\n",
    "            return self.sentiment\n",
    "        else: \n",
    "            return self.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "file = r\"C:\\Users\\fathu\\Documents\\Sentiment Analysis Data\\database.json\"\n",
    "reviews = []\n",
    "\n",
    "with open(file) as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        reviews.append(Reviews(review['reviewText'], review['overall'] ,review['sentiment']))\n",
    "\n",
    "negative = list(filter(lambda x: x.sentiment == 'NEGATIVE', reviews))\n",
    "positive = list(filter(lambda x: x.sentiment == 'POSITIVE', reviews))\n",
    "data = negative + positive\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PREP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn import svm\\nimport numpy as np\\nimport pandas as pd'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn import svm\\nimport numpy as np\\nimport pandas as pd'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "X = [x.text for x in data]\n",
    "\n",
    "df = pd.DataFrame({'text':X})\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*>\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "cleaned = lambda x: text_cleaning(x)\n",
    "df['cleaned_text'] = pd.DataFrame(df['text'].apply(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df['cleaned_text'].values.tolist()\n",
    "y = [x.get_sentiment() for x in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is my experience with the game and the staff  read and judge for yourselvesfirst of all this game is fun  it is repetitive in its mechanics and game play  the tournaments where you can test yourself against other players characters and climb the leader board are the highlight to me  there do appear to be quite a few glitches and exploits and this appears to be a newer game so that isnt unexpectedive dealt with customer service twice since installthe first glitch i encountered was the result of an in app purchase  they got my money i didnt get my gemstones  i hit the customer service button under the gear icon menu after you launch the game and opened a ticket stating my problem and asking what information was needed from me  the first response relaying instructions was prompt  the second in which i received my merchandise took three days  at this point i was annoyed and thinking about writing up a scathing hate review when my email informs me that gameloft has responded  not only had they restored my items but they gave me over ten times what i paid for originally presumably for waiting so long  nothing placates rage quite like free stuffmy second encounter with the staff resulted from exploits  i am not typically a cheater  while playing the game  i noticed that one of the mechanics was being abused by over half of my competition so i  decided i would discover this glitch and use it as well  when in rome and so i figured it out  fewer than  hour later i  found my account inaccessible  this tells me a couple of things  the staff reads the complaints about individual cheaters and they care about making their game betterin regards to data mining and analytics read another review because i have no idearather than pouting im choosing to spend my banishment from sticks and ponies relating to you my experience with this app  you may have had a less favorable reaction had you been in my shoes so make of this what you willi gave  stars for two   i will probably never look at this review again  but i anticipate its   no one reads  star reviews\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_x_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "test_x_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train[0])\n",
    "print(train_x_vectors[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn import svm\\nimport pandas as pd'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn import svm\\nimport numpy as np\\nimport pandas as pd'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fathu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.86084017 0.86412979 0.8455374  0.86749064 0.83495396 0.86749064\n",
      " 0.8303771  0.86749064 0.82672977 0.86749064 0.63722719 0.65732181\n",
      " 0.68092049 0.6889301  0.70180221 0.70616459 0.63272237 0.65188747\n",
      " 0.6677616  0.68084942 0.69393664 0.70366175        nan 0.85869484\n",
      "        nan 0.86248489        nan 0.86162698        nan 0.8554057\n",
      "        nan 0.84610918        nan 0.84339183        nan 0.83938706\n",
      "        nan 0.838958          nan 0.83774254        nan 0.83931563]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best models Pipeline(steps=[('classifier', SVC(C=4))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "svc = svm.SVC()\n",
    "pipe = Pipeline([(\"classifier\", svc)])\n",
    "\n",
    "\n",
    "params = [\n",
    "    {\"classifier\": [svc],\n",
    "    \"classifier__kernel\": ['linear', 'rbf'],\n",
    "    \"classifier__C\": [1,4,8,16,32]\n",
    "    },\n",
    "    {\"classifier\": [DecisionTreeClassifier()],\n",
    "    \"classifier__criterion\": ['gini', 'entropy'],\n",
    "    \"classifier__max_depth\": [2,4,6,8,10,12]\n",
    "    },\n",
    "    {\"classifier\": [LogisticRegression()],\n",
    "    \"classifier__penalty\": ['l1', 'l2'],\n",
    "    \"classifier__C\": np.logspace(0, 4, 10)\n",
    "    }\n",
    "]\n",
    "\n",
    "gridsearch = GridSearchCV(pipe, params, cv = 5, verbose=0, n_jobs=-1)\n",
    "best_model = gridsearch.fit(train_x_vectors, y_train)\n",
    "\n",
    "print(f'best models {best_model.best_estimator_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean accuracy: 0.8715382048715382\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.87      0.88      0.87      3038\n",
      "    POSITIVE       0.87      0.87      0.87      2956\n",
      "\n",
      "    accuracy                           0.87      5994\n",
      "   macro avg       0.87      0.87      0.87      5994\n",
      "weighted avg       0.87      0.87      0.87      5994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"the mean accuracy:\", best_model.score(test_x_vectors, y_test))\n",
    "print(classification_report(y_test,best_model.best_estimator_.predict(test_x_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVING MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sentiment_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42af8fc72df09895d8fa0abc08bbacec0328e41a9be9da013dc1ad879d313737"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
